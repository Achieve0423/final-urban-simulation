{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d571b0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Information:\n",
      "Number of nodes: 401\n",
      "Number of edges: 467\n",
      "\n",
      "Flow Data Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 61474 entries, 0 to 61473\n",
      "Data columns (total 6 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   station_origin       61474 non-null  object \n",
      " 1   station_destination  61474 non-null  object \n",
      " 2   flows                61474 non-null  int64  \n",
      " 3   population           61474 non-null  int64  \n",
      " 4   jobs                 61474 non-null  int64  \n",
      " 5   distance             61474 non-null  float64\n",
      "dtypes: float64(1), int64(3), object(2)\n",
      "memory usage: 2.8+ MB\n",
      "None\n",
      "\n",
      "All stations in the flow data are present in the network.\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the network data\n",
    "G = nx.read_graphml('london.graphml')\n",
    "\n",
    "# Load the flow data\n",
    "flows_df = pd.read_csv('london_flows.csv')\n",
    "\n",
    "# Basic network information\n",
    "print(\"Network Information:\")\n",
    "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {G.number_of_edges()}\")\n",
    "\n",
    "# Basic flow data information\n",
    "print(\"\\nFlow Data Information:\")\n",
    "print(flows_df.info())\n",
    "\n",
    "# Check for missing stations\n",
    "network_stations = set(G.nodes())\n",
    "flow_stations = set(flows_df['station_origin'].unique()) | set(flows_df['station_destination'].unique())\n",
    "missing_stations = flow_stations - network_stations\n",
    "\n",
    "if missing_stations:\n",
    "    print(f\"\\nWarning: The following stations are in the flow data but not in the network: {missing_stations}\")\n",
    "else:\n",
    "    print(\"\\nAll stations in the flow data are present in the network.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7633cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 stations by Degree Centrality:\n",
      "                          Degree  Betweenness  Closeness\n",
      "Stratford                 0.0225     0.297846   0.104384\n",
      "Bank and Monument         0.0200     0.290489   0.113572\n",
      "King's Cross St. Pancras  0.0175     0.255307   0.113443\n",
      "Baker Street              0.0175     0.191568   0.108962\n",
      "Earl's Court              0.0150     0.125892   0.090416\n",
      "Oxford Circus             0.0150     0.053844   0.111204\n",
      "Liverpool Street          0.0150     0.270807   0.110254\n",
      "Waterloo                  0.0150     0.243921   0.112265\n",
      "Green Park                0.0150     0.215835   0.114778\n",
      "Canning Town              0.0150     0.096167   0.091575\n",
      "\n",
      "Top 10 stations by Betweenness Centrality:\n",
      "                          Degree  Betweenness  Closeness\n",
      "Stratford                 0.0225     0.297846   0.104384\n",
      "Bank and Monument         0.0200     0.290489   0.113572\n",
      "Liverpool Street          0.0150     0.270807   0.110254\n",
      "King's Cross St. Pancras  0.0175     0.255307   0.113443\n",
      "Waterloo                  0.0150     0.243921   0.112265\n",
      "Green Park                0.0150     0.215835   0.114778\n",
      "Euston                    0.0125     0.208324   0.109830\n",
      "Westminster               0.0100     0.203335   0.112549\n",
      "Baker Street              0.0175     0.191568   0.108962\n",
      "Finchley Road             0.0100     0.165085   0.102617\n",
      "\n",
      "Top 10 stations by Closeness Centrality:\n",
      "                          Degree  Betweenness  Closeness\n",
      "Green Park                0.0150     0.215835   0.114778\n",
      "Bank and Monument         0.0200     0.290489   0.113572\n",
      "King's Cross St. Pancras  0.0175     0.255307   0.113443\n",
      "Westminster               0.0100     0.203335   0.112549\n",
      "Waterloo                  0.0150     0.243921   0.112265\n",
      "Oxford Circus             0.0150     0.053844   0.111204\n",
      "Bond Street               0.0100     0.141591   0.110988\n",
      "Farringdon                0.0050     0.064110   0.110742\n",
      "Angel                     0.0050     0.064110   0.110742\n",
      "Moorgate                  0.0100     0.130169   0.110314\n"
     ]
    }
   ],
   "source": [
    "# Calculate centrality measures\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "\n",
    "# Create a DataFrame with centrality measures\n",
    "centrality_df = pd.DataFrame({\n",
    "    'Degree': degree_centrality,\n",
    "    'Betweenness': betweenness_centrality,\n",
    "    'Closeness': closeness_centrality\n",
    "})\n",
    "\n",
    "# Display top 10 stations by each centrality measure\n",
    "print(\"\\nTop 10 stations by Degree Centrality:\")\n",
    "print(centrality_df.sort_values('Degree', ascending=False).head(10))\n",
    "\n",
    "print(\"\\nTop 10 stations by Betweenness Centrality:\")\n",
    "print(centrality_df.sort_values('Betweenness', ascending=False).head(10))\n",
    "\n",
    "print(\"\\nTop 10 stations by Closeness Centrality:\")\n",
    "print(centrality_df.sort_values('Closeness', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2767468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Impact of removing top 10 stations by degree centrality:\n",
      "                    Station       ASPL  Efficiency  Largest Component Size\n",
      "0                 Stratford        inf    0.000000                     379\n",
      "1         Bank and Monument  14.130739    0.070768                     400\n",
      "2  King's Cross St. Pancras  14.250890    0.070171                     400\n",
      "3              Baker Street  14.384624    0.069519                     400\n",
      "4              Earl's Court  13.832018    0.072296                     400\n",
      "5             Oxford Circus  13.614925    0.073449                     400\n",
      "6          Liverpool Street  14.100338    0.070920                     400\n",
      "7                  Waterloo  13.960802    0.071629                     400\n",
      "8                Green Park  13.824536    0.072335                     400\n",
      "9              Canning Town        inf    0.000000                     387\n"
     ]
    }
   ],
   "source": [
    "def remove_node(G, node):\n",
    "    G_copy = G.copy()\n",
    "    G_copy.remove_node(node)\n",
    "    return G_copy\n",
    "\n",
    "def calculate_network_metrics(G):\n",
    "    if not nx.is_connected(G):\n",
    "        return float('inf'), 0, len(max(nx.connected_components(G), key=len))\n",
    "    else:\n",
    "        aspl = nx.average_shortest_path_length(G)\n",
    "        efficiency = 1 / aspl\n",
    "        return aspl, efficiency, G.number_of_nodes()\n",
    "\n",
    "# Perform node removal for top 10 stations by degree centrality\n",
    "top_stations = centrality_df.sort_values('Degree', ascending=False).head(10).index\n",
    "\n",
    "results = []\n",
    "for station in top_stations:\n",
    "    G_removed = remove_node(G, station)\n",
    "    aspl, efficiency, component_size = calculate_network_metrics(G_removed)\n",
    "    results.append({\n",
    "        'Station': station,\n",
    "        'ASPL': aspl,\n",
    "        'Efficiency': efficiency,\n",
    "        'Largest Component Size': component_size\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nImpact of removing top 10 stations by degree centrality:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13dc5ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitted gravity model parameters: K = 1.00e-05, beta = 1.00\n",
      "R-squared: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c7/kb9rb69966j3v_4115qt2zxh0000gn/T/ipykernel_22029/1098051352.py:5: RuntimeWarning: divide by zero encountered in divide\n",
      "  return K * Pi * Pj / (d ** beta)\n",
      "/Users/junrunchen/anaconda3/lib/python3.11/site-packages/scipy/optimize/_minpack_py.py:1010: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def gravity_model(X, K, beta):\n",
    "    Pi, Pj, d = X\n",
    "    return K * Pi * Pj / (d ** beta)\n",
    "\n",
    "# Prepare data\n",
    "X = flows_df[['population', 'jobs', 'distance']].values.T\n",
    "y = flows_df['flows'].values\n",
    "\n",
    "# Fit the model\n",
    "popt, _ = curve_fit(gravity_model, X, y, p0=[1e-5, 1])\n",
    "\n",
    "K_fit, beta_fit = popt\n",
    "print(f\"\\nFitted gravity model parameters: K = {K_fit:.2e}, beta = {beta_fit:.2f}\")\n",
    "\n",
    "# Calculate predicted flows\n",
    "flows_df['predicted_flows'] = gravity_model(X, K_fit, beta_fit)\n",
    "\n",
    "# Calculate R-squared\n",
    "ss_res = np.sum((flows_df['flows'] - flows_df['predicted_flows'])**2)\n",
    "ss_tot = np.sum((flows_df['flows'] - flows_df['flows'].mean())**2)\n",
    "r_squared = 1 - (ss_res / ss_tot)\n",
    "\n",
    "print(f\"R-squared: {r_squared:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94fcefe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Base Flows: 7.48084457877715e-31\n",
      "Total Scenario A Flows: 7.4808445787771495e-31\n",
      "Total Scenario B1 Flows: 1.3274844971157016e-34\n",
      "Total Scenario B2 Flows: 2.35580144512799e-38\n",
      "Detailed Results by Alpha Variation: {0.001: {'Base': 9.842316840059216e-31, 'Scenario A': 1.3892922807262048e-43, 'Scenario B1': 1.7466075540273084e-34, 'Scenario B2': 3.0996802086779167e-38}, 0.01: {'Base': 7.48084457877715e-31, 'Scenario A': 2.760153869557147e-44, 'Scenario B1': 1.3274844971157016e-34, 'Scenario B2': 2.35580144512799e-38}, 0.1: {'Base': 5.195368955947039e-32, 'Scenario A': 2.651339031287365e-51, 'Scenario B1': 9.199470357202758e-36, 'Scenario B2': 1.630313553049657e-39}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "flows_data = pd.read_csv('london_flows.csv')\n",
    "flows_data['intervening_opportunities'] = flows_data['jobs'] / flows_data['distance']\n",
    "canary_wharf_mask = flows_data['station_destination'] == 'Canary Wharf'\n",
    "\n",
    "# Define the Intervening Opportunities Model function\n",
    "def intervening_opportunities_model(population, jobs, distance, opportunities, b, alpha):\n",
    "    \"\"\"Calculate commuter flows based on population, jobs, distance, and intervening opportunities.\"\"\"\n",
    "    return population * jobs * np.exp(-b * distance - alpha * opportunities)\n",
    "\n",
    "# best values for b and alpha\n",
    "b_fit_refit = 0.348\n",
    "alpha_initial = 0.01\n",
    "\n",
    "# Calculate base scenario flows\n",
    "flows_data['base_flows'] = intervening_opportunities_model(\n",
    "    flows_data['population'], flows_data['jobs'], flows_data['distance'],\n",
    "    flows_data['intervening_opportunities'], b_fit_refit, alpha_initial\n",
    ")\n",
    "\n",
    "# Calculate total commuters for baseline\n",
    "total_commuters_initial = flows_data['base_flows'].sum()\n",
    "\n",
    "# Scenario A: 50% job reduction at Canary Wharf\n",
    "flows_data.loc[canary_wharf_mask, 'jobs'] *= 0.5\n",
    "flows_data['scenario_a_flows'] = intervening_opportunities_model(\n",
    "    flows_data['population'], flows_data['jobs'], flows_data['distance'],\n",
    "    flows_data['intervening_opportunities'], b_fit_refit, alpha_initial\n",
    ")\n",
    "\n",
    "# Reset jobs to original for accurate scenario B calculations\n",
    "flows_data.loc[canary_wharf_mask, 'jobs'] = flows_data.loc[canary_wharf_mask, 'jobs'] / 0.5\n",
    "\n",
    "# Verify total commuters in Scenario A\n",
    "total_commuters_scenario_a = flows_data['scenario_a_flows'].sum()\n",
    "\n",
    "# Adjust flows to conserve total commuters if needed\n",
    "if total_commuters_initial != total_commuters_scenario_a:\n",
    "    adjustment_factor = total_commuters_initial / total_commuters_scenario_a\n",
    "    flows_data['scenario_a_flows'] *= adjustment_factor\n",
    "\n",
    "# Scenario B: Increase in transport cost by adjusting b by 10% and 20%\n",
    "for i, increase in enumerate([1.10, 1.20], start=1):\n",
    "    adjusted_b = b_fit_refit * increase\n",
    "    flows_data[f'scenario_b{i}_flows'] = intervening_opportunities_model(\n",
    "        flows_data['population'], flows_data['jobs'], flows_data['distance'],\n",
    "        flows_data['intervening_opportunities'], adjusted_b, alpha_initial\n",
    "    )\n",
    "\n",
    "# Print out total flows for each scenario to verify changes\n",
    "print(\"Total Base Flows:\", total_commuters_initial)\n",
    "print(\"Total Scenario A Flows:\", flows_data['scenario_a_flows'].sum())\n",
    "print(\"Total Scenario B1 Flows:\", flows_data['scenario_b1_flows'].sum())\n",
    "print(\"Total Scenario B2 Flows:\", flows_data['scenario_b2_flows'].sum())\n",
    "\n",
    "# Sensitivity Analysis with different alpha values\n",
    "alpha_values = [0.001, 0.01, 0.1]\n",
    "results = {}\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    flows_data['scenario_flows'] = intervening_opportunities_model(\n",
    "        flows_data['population'], flows_data['jobs'], flows_data['distance'],\n",
    "        flows_data['intervening_opportunities'], b_fit_refit, alpha\n",
    "    )\n",
    "    flows_data[f'scenario_a_flows_alpha_{alpha}'] = intervening_opportunities_model(\n",
    "        flows_data['population'], flows_data.loc[canary_wharf_mask, 'jobs'] * 0.5, flows_data['distance'],\n",
    "        flows_data['intervening_opportunities'], b_fit_refit, alpha\n",
    "    )\n",
    "    flows_data[f'scenario_b1_flows_alpha_{alpha}'] = intervening_opportunities_model(\n",
    "        flows_data['population'], flows_data['jobs'], flows_data['distance'],\n",
    "        flows_data['intervening_opportunities'], b_fit_refit * 1.10, alpha\n",
    "    )\n",
    "    flows_data[f'scenario_b2_flows_alpha_{alpha}'] = intervening_opportunities_model(\n",
    "        flows_data['population'], flows_data['jobs'], flows_data['distance'],\n",
    "        flows_data['intervening_opportunities'], b_fit_refit * 1.20, alpha\n",
    "    )\n",
    "\n",
    "    # Collect results for analysis\n",
    "    results[alpha] = {\n",
    "        'Base': flows_data['scenario_flows'].sum(),\n",
    "        'Scenario A': flows_data[f'scenario_a_flows_alpha_{alpha}'].sum(),\n",
    "        'Scenario B1': flows_data[f'scenario_b1_flows_alpha_{alpha}'].sum(),\n",
    "        'Scenario B2': flows_data[f'scenario_b2_flows_alpha_{alpha}'].sum()\n",
    "    }\n",
    "\n",
    "# Output results for each alpha variation\n",
    "print(\"Detailed Results by Alpha Variation:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d711c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Impact based on Degree centrality:\n",
      "Node: Stratford\n",
      "  Efficiency change: 0.1013\n",
      "  Largest component size change: 22\n",
      "  Flow impact: 0.0747\n",
      "Node: Bank and Monument\n",
      "  Efficiency change: 0.0889\n",
      "  Largest component size change: 1\n",
      "  Flow impact: 0.0700\n",
      "Node: Baker Street\n",
      "  Efficiency change: 0.0859\n",
      "  Largest component size change: 1\n",
      "  Flow impact: 0.0145\n",
      "Node: King's Cross St. Pancras\n",
      "  Efficiency change: 0.0820\n",
      "  Largest component size change: 3\n",
      "  Flow impact: 0.0400\n",
      "Node: West Ham\n",
      "  Efficiency change: 0.0757\n",
      "  Largest component size change: 3\n",
      "  Flow impact: 0.0093\n",
      "\n",
      "Impact based on Betweenness centrality:\n",
      "Node: Bank and Monument\n",
      "  Efficiency change: 0.0045\n",
      "  Largest component size change: 1\n",
      "  Flow impact: 0.0700\n",
      "Node: Stratford\n",
      "  Efficiency change: 0.0967\n",
      "  Largest component size change: 22\n",
      "  Flow impact: 0.0747\n",
      "Node: King's Cross St. Pancras\n",
      "  Efficiency change: 0.0859\n",
      "  Largest component size change: 1\n",
      "  Flow impact: 0.0400\n",
      "Node: Liverpool Street\n",
      "  Efficiency change: 0.0803\n",
      "  Largest component size change: 6\n",
      "  Flow impact: 0.0600\n",
      "Node: Moorgate\n",
      "  Efficiency change: 0.0785\n",
      "  Largest component size change: 0\n",
      "  Flow impact: 0.0188\n",
      "\n",
      "Impact based on Closeness centrality:\n",
      "Node: Green Park\n",
      "  Efficiency change: 0.0021\n",
      "  Largest component size change: 1\n",
      "  Flow impact: 0.0187\n",
      "Node: Bank and Monument\n",
      "  Efficiency change: 0.0043\n",
      "  Largest component size change: 1\n",
      "  Flow impact: 0.0700\n",
      "Node: King's Cross St. Pancras\n",
      "  Efficiency change: 0.0069\n",
      "  Largest component size change: 1\n",
      "  Flow impact: 0.0400\n",
      "Node: Westminster\n",
      "  Efficiency change: 0.0006\n",
      "  Largest component size change: 1\n",
      "  Flow impact: 0.0110\n",
      "Node: Waterloo\n",
      "  Efficiency change: 0.0025\n",
      "  Largest component size change: 1\n",
      "  Flow impact: 0.0589\n",
      "\n",
      "Impact based on Weighted Degree centrality:\n",
      "Node: Stratford\n",
      "  Efficiency change: 0.1013\n",
      "  Largest component size change: 22\n",
      "  Flow impact: 0.0747\n",
      "Node: Bank and Monument\n",
      "  Efficiency change: 0.0889\n",
      "  Largest component size change: 1\n",
      "  Flow impact: 0.0700\n",
      "Node: Liverpool Street\n",
      "  Efficiency change: 0.0859\n",
      "  Largest component size change: 1\n",
      "  Flow impact: 0.0600\n",
      "Node: Waterloo\n",
      "  Efficiency change: 0.0850\n",
      "  Largest component size change: 1\n",
      "  Flow impact: 0.0589\n",
      "Node: Canary Wharf\n",
      "  Efficiency change: 0.0828\n",
      "  Largest component size change: 11\n",
      "  Flow impact: 0.0476\n",
      "\n",
      "Best centrality measure for identifying critical stations: Weighted Degree\n",
      "Total efficiency changes:\n",
      "  Degree: 0.7745\n",
      "  Betweenness: 0.7336\n",
      "  Closeness: 0.1013\n",
      "  Weighted Degree: 0.7976\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def calculate_centralities(G, flows_df):\n",
    "    # 度中心性\n",
    "    degree_cent = nx.degree_centrality(G)\n",
    "    \n",
    "    # 加权介数中心性\n",
    "    edge_weights = {(row['station_origin'], row['station_destination']): 1/(row['flows'] + 1) \n",
    "                    for _, row in flows_df.iterrows()}\n",
    "    nx.set_edge_attributes(G, edge_weights, 'weight')\n",
    "    betweenness_cent = nx.betweenness_centrality(G, weight='weight')\n",
    "    \n",
    "    # 接近中心性\n",
    "    closeness_cent = nx.closeness_centrality(G)\n",
    "    \n",
    "    # 基于流量的加权度中心性\n",
    "    weighted_degree = defaultdict(float)\n",
    "    for _, row in flows_df.iterrows():\n",
    "        weighted_degree[row['station_origin']] += row['flows']\n",
    "        weighted_degree[row['station_destination']] += row['flows']\n",
    "    max_flow = max(weighted_degree.values())\n",
    "    weighted_degree = {k: v/max_flow for k, v in weighted_degree.items()}\n",
    "    \n",
    "    return degree_cent, betweenness_cent, closeness_cent, weighted_degree\n",
    "\n",
    "def assess_removal_impact(G, centrality, flows_df, top_n=10):\n",
    "    sorted_nodes = sorted(centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "    impacts = []\n",
    "    G_copy = G.copy()\n",
    "    total_flow = flows_df['flows'].sum()\n",
    "    \n",
    "    for node, _ in sorted_nodes[:top_n]:\n",
    "        if node not in G_copy:\n",
    "            continue\n",
    "        \n",
    "        efficiency_before = nx.global_efficiency(G_copy)\n",
    "        largest_cc_before = len(max(nx.connected_components(G_copy), key=len))\n",
    "        \n",
    "        # 计算受影响的流量\n",
    "        affected_flows = flows_df[(flows_df['station_origin'] == node) | \n",
    "                                  (flows_df['station_destination'] == node)]\n",
    "        flow_impact = affected_flows['flows'].sum() / total_flow\n",
    "        \n",
    "        G_copy.remove_node(node)\n",
    "        \n",
    "        if nx.is_connected(G_copy):\n",
    "            efficiency_after = nx.global_efficiency(G_copy)\n",
    "        else:\n",
    "            efficiency_after = 0\n",
    "        largest_cc_after = len(max(nx.connected_components(G_copy), key=len))\n",
    "        \n",
    "        impacts.append({\n",
    "            'node': node,\n",
    "            'efficiency_change': efficiency_before - efficiency_after,\n",
    "            'cc_size_change': largest_cc_before - largest_cc_after,\n",
    "            'flow_impact': flow_impact\n",
    "        })\n",
    "    \n",
    "    return impacts\n",
    "\n",
    "# 执行分析\n",
    "degree_cent, betweenness_cent, closeness_cent, weighted_degree = calculate_centralities(G, flows_df)\n",
    "\n",
    "# 评估不同中心性指标的移除影响\n",
    "degree_impact = assess_removal_impact(G, degree_cent, flows_df)\n",
    "betweenness_impact = assess_removal_impact(G, betweenness_cent, flows_df)\n",
    "closeness_impact = assess_removal_impact(G, closeness_cent, flows_df)\n",
    "weighted_impact = assess_removal_impact(G, weighted_degree, flows_df)\n",
    "\n",
    "# 打印结果\n",
    "for impact_type, impacts in [(\"Degree\", degree_impact), \n",
    "                             (\"Betweenness\", betweenness_impact),\n",
    "                             (\"Closeness\", closeness_impact),\n",
    "                             (\"Weighted Degree\", weighted_impact)]:\n",
    "    print(f\"\\nImpact based on {impact_type} centrality:\")\n",
    "    for impact in impacts[:5]:\n",
    "        print(f\"Node: {impact['node']}\")\n",
    "        print(f\"  Efficiency change: {impact['efficiency_change']:.4f}\")\n",
    "        print(f\"  Largest component size change: {impact['cc_size_change']}\")\n",
    "        print(f\"  Flow impact: {impact['flow_impact']:.4f}\")\n",
    "\n",
    "# 分析哪种中心性指标更好\n",
    "total_efficiency_change = {\n",
    "    \"Degree\": sum(impact['efficiency_change'] for impact in degree_impact),\n",
    "    \"Betweenness\": sum(impact['efficiency_change'] for impact in betweenness_impact),\n",
    "    \"Closeness\": sum(impact['efficiency_change'] for impact in closeness_impact),\n",
    "    \"Weighted Degree\": sum(impact['efficiency_change'] for impact in weighted_impact)\n",
    "}\n",
    "\n",
    "best_centrality = max(total_efficiency_change, key=total_efficiency_change.get)\n",
    "print(f\"\\nBest centrality measure for identifying critical stations: {best_centrality}\")\n",
    "print(\"Total efficiency changes:\")\n",
    "for centrality, change in total_efficiency_change.items():\n",
    "    print(f\"  {centrality}: {change:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e228a7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 stations by Weighted Degree Centrality:\n",
      "Stratford: 1.0000\n",
      "Bank and Monument: 0.9373\n",
      "Liverpool Street: 0.8033\n",
      "Waterloo: 0.7881\n",
      "Canary Wharf: 0.6368\n",
      "Victoria: 0.6140\n",
      "London Bridge: 0.5425\n",
      "King's Cross St. Pancras: 0.5347\n",
      "Highbury & Islington: 0.4260\n",
      "Canada Water: 0.4118\n",
      "\n",
      "Impact of removing top 3 stations (Weighted Network):\n",
      "Node: Stratford\n",
      "  Efficiency change: 1075.0999\n",
      "  Flow disruption: 0.0747\n",
      "Node: Bank and Monument\n",
      "  Efficiency change: 4840.3601\n",
      "  Flow disruption: 0.0700\n",
      "Node: Liverpool Street\n",
      "  Efficiency change: 1021.7888\n",
      "  Flow disruption: 0.0600\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def calculate_weighted_centralities(G, flows_df):\n",
    "    # 加权度中心性\n",
    "    weighted_degree = defaultdict(float)\n",
    "    for _, row in flows_df.iterrows():\n",
    "        if row['station_origin'] in G and row['station_destination'] in G:\n",
    "            weighted_degree[row['station_origin']] += row['flows']\n",
    "            weighted_degree[row['station_destination']] += row['flows']\n",
    "    \n",
    "    max_flow = max(weighted_degree.values()) if weighted_degree else 1\n",
    "    weighted_degree = {k: v/max_flow for k, v in weighted_degree.items()}\n",
    "    \n",
    "    # 加权介数中心性\n",
    "    edge_weights = {(row['station_origin'], row['station_destination']): 1/(row['flows'] + 1) \n",
    "                    for _, row in flows_df.iterrows() \n",
    "                    if row['station_origin'] in G and row['station_destination'] in G}\n",
    "    nx.set_edge_attributes(G, edge_weights, 'weight')\n",
    "    weighted_betweenness = nx.betweenness_centrality(G, weight='weight')\n",
    "    \n",
    "    # 加权接近中心性\n",
    "    weighted_closeness = nx.closeness_centrality(G, distance='weight')\n",
    "    \n",
    "    return weighted_degree, weighted_betweenness, weighted_closeness\n",
    "\n",
    "def weighted_global_efficiency(G, flows_df):\n",
    "    total_efficiency = 0\n",
    "    total_possible_paths = 0\n",
    "    \n",
    "    for _, row in flows_df.iterrows():\n",
    "        if row['station_origin'] in G and row['station_destination'] in G and row['station_origin'] != row['station_destination']:\n",
    "            total_possible_paths += 1\n",
    "            if nx.has_path(G, row['station_origin'], row['station_destination']):\n",
    "                try:\n",
    "                    path_length = nx.shortest_path_length(G, row['station_origin'], row['station_destination'], weight='weight')\n",
    "                    if path_length > 0:\n",
    "                        total_efficiency += row['flows'] / path_length\n",
    "                    else:\n",
    "                        total_efficiency += row['flows']  # 如果路径长度为0，我们假设效率最大\n",
    "                except nx.NetworkXNoPath:\n",
    "                    pass  # 如果没有路径，我们不增加效率\n",
    "    \n",
    "    return total_efficiency / total_possible_paths if total_possible_paths > 0 else 0\n",
    "\n",
    "def flow_disruption(G, node, flows_df):\n",
    "    affected_flows = flows_df[(flows_df['station_origin'] == node) | (flows_df['station_destination'] == node)]\n",
    "    return affected_flows['flows'].sum() / flows_df['flows'].sum()\n",
    "\n",
    "def assess_weighted_removal_impact(G, centrality, flows_df, top_n=3):\n",
    "    sorted_nodes = sorted(centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "    impacts = []\n",
    "    \n",
    "    efficiency_before = weighted_global_efficiency(G, flows_df)\n",
    "    \n",
    "    for node, _ in sorted_nodes[:top_n]:\n",
    "        if node not in G:\n",
    "            continue\n",
    "        flow_disruption_value = flow_disruption(G, node, flows_df)\n",
    "        \n",
    "        G_copy = G.copy()\n",
    "        G_copy.remove_node(node)\n",
    "        \n",
    "        efficiency_after = weighted_global_efficiency(G_copy, flows_df)\n",
    "        \n",
    "        impacts.append({\n",
    "            'node': node,\n",
    "            'efficiency_change': efficiency_before - efficiency_after,\n",
    "            'flow_disruption': flow_disruption_value\n",
    "        })\n",
    "    \n",
    "    return impacts\n",
    "\n",
    "# 计算加权中心性\n",
    "weighted_degree, weighted_betweenness, weighted_closeness = calculate_weighted_centralities(G, flows_df)\n",
    "\n",
    "# 打印加权度中心性的前10个节点\n",
    "print(\"Top 10 stations by Weighted Degree Centrality:\")\n",
    "for node, centrality in sorted(weighted_degree.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    print(f\"{node}: {centrality:.4f}\")\n",
    "\n",
    "# 评估移除影响\n",
    "weighted_impacts = assess_weighted_removal_impact(G, weighted_degree, flows_df)\n",
    "\n",
    "print(\"\\nImpact of removing top 3 stations (Weighted Network):\")\n",
    "for impact in weighted_impacts:\n",
    "    print(f\"Node: {impact['node']}\")\n",
    "    print(f\"  Efficiency change: {impact['efficiency_change']:.4f}\")\n",
    "    print(f\"  Flow disruption: {impact['flow_disruption']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ad84d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
